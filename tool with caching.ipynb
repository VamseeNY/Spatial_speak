{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "c:\\Users\\vamsv\\dbs\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\vamsv\\AppData\\Local\\Temp\\ipykernel_30072\\332770101.py:80: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"llama3.2:3b\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted topic: quantum computing\n",
      "Successfully processed: https://qiskit.org/learn/\n",
      "Successfully processed: https://quantum.microsoft.com/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vamsv\\AppData\\Local\\Temp\\ipykernel_30072\\332770101.py:130: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
      "C:\\Users\\vamsv\\AppData\\Local\\Temp\\ipykernel_30072\\332770101.py:152: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  context_docs = retriever.get_relevant_documents(question)\n",
      "c:\\Users\\vamsv\\dbs\\lib\\site-packages\\langsmith\\client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "C:\\Users\\vamsv\\AppData\\Local\\Temp\\ipykernel_30072\\332770101.py:99: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\n",
      "C:\\Users\\vamsv\\AppData\\Local\\Temp\\ipykernel_30072\\332770101.py:100: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  combine_docs_chain = StuffDocumentsChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 1: Quantum computing is a type of computing that uses quantum mechanics to perform calculations, leveraging key concepts such as superposition, entanglement, and interference to solve complex problems. It has various applications, including cryptography, optimization, and machine learning, with the goal of enabling breakthroughs in fields like chemistry, materials science, and pharmaceutical research. Quantum computers have the potential to dramatically reduce noise effects and enable highly reliable logical qubits.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain import hub\n",
    "import trafilatura\n",
    "\n",
    "# For SOTA topic extraction using sentence transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Predefined list of webpages (format: {topic: [urls]})\n",
    "PREDEFINED_URLS = {\n",
    "    \"AI safety\": [\"https://www.deepmind.com/about/safety-and-security\",  #scrapping blocked on this shit\n",
    "                  \"https://openai.com/safety\"],\n",
    "    \"quantum computing\": [\"https://qiskit.org/learn/\", #ts works\n",
    "                           \"https://quantum.microsoft.com/\"]\n",
    "}\n",
    "# List of topics for similarity comparisons\n",
    "PREDEFINED_TOPICS = list(PREDEFINED_URLS.keys())\n",
    "\n",
    "# Initialize the SOTA embedding model for topic extraction\n",
    "topic_extractor = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def extract_topic(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the most semantically similar topic from the predefined topics.\n",
    "    Returns the topic if the similarity score exceeds a threshold, else None.\n",
    "    \"\"\"\n",
    "    query_embedding = topic_extractor.encode(query, convert_to_tensor=True)\n",
    "    topics_embeddings = topic_extractor.encode(PREDEFINED_TOPICS, convert_to_tensor=True)\n",
    "    cosine_scores = util.cos_sim(query_embedding, topics_embeddings)\n",
    "    best_score, best_index = torch.max(cosine_scores, dim=1)\n",
    "    # Set a threshold (e.g., 0.6) for considering the topic as a match\n",
    "    if best_score.item() >= 0.6:\n",
    "        return PREDEFINED_TOPICS[best_index.item()]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def web_search(query):\n",
    "    \"\"\"Fallback web search using DuckDuckGo\"\"\"\n",
    "    search = DuckDuckGoSearchAPIWrapper()\n",
    "    return search.results(query, 3)  # Get top 3 results\n",
    "\n",
    "from typing import List\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def process_urls(urls: List[str]) -> List[Document]:\n",
    "    \"\"\"Process URLs into document chunks with error handling\"\"\"\n",
    "    documents = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            downloaded = trafilatura.fetch_url(url)\n",
    "            text = trafilatura.extract(downloaded)\n",
    "            if text:\n",
    "                documents.append(Document(\n",
    "                    page_content=text,\n",
    "                    metadata={\"source\": url}\n",
    "                ))\n",
    "                print(f\"Successfully processed: {url}\")\n",
    "            else:\n",
    "                print(f\"No content found at: {url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {str(e)}\")\n",
    "    \n",
    "    if not documents:\n",
    "        raise ValueError(\"No valid documents found from the provided URLs\")\n",
    "    \n",
    "    # Split documents into manageable chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, chunk_overlap=200\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "# Initialize LLM (Make sure Ollama is running)\n",
    "llm = Ollama(model=\"llama3.2:3b\")\n",
    "\n",
    "def answer_with_context(question: str, context_docs: List[Document]):\n",
    "    \"\"\"Generate answer using context documents\"\"\"\n",
    "    if not context_docs:\n",
    "        return \"No relevant information found to answer this question.\"\n",
    "    \n",
    "    # Create retrieval chain\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "    try:\n",
    "        vectorstore = FAISS.from_documents(context_docs, embeddings)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {str(e)}\")\n",
    "        return \"Failed to process documents for answering.\"\n",
    "    \n",
    "    retriever = vectorstore.as_retriever(k=3)\n",
    "    \n",
    "    # Create QA chain\n",
    "    qa_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\n",
    "    combine_docs_chain = StuffDocumentsChain(\n",
    "        llm_chain=qa_chain, document_variable_name=\"context\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = combine_docs_chain.invoke({\n",
    "            \"input_documents\": context_docs,\n",
    "            \"question\": question\n",
    "        })\n",
    "        return result[\"output_text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer: {str(e)}\")\n",
    "        return \"Failed to generate answer.\"\n",
    "\n",
    "# --- Caching Strategy Implementation ---\n",
    "\n",
    "# Cache for topic-based FAISS vector stores\n",
    "VECTOR_STORE_CACHE = {}\n",
    "\n",
    "def get_vector_store_for_topic(topic: str, urls: List[str]):\n",
    "    \"\"\"\n",
    "    Retrieve or create a FAISS vector store for the given topic.\n",
    "    Uses caching to avoid re-vectorizing documents for repeated topics.\n",
    "    \"\"\"\n",
    "    if topic in VECTOR_STORE_CACHE:\n",
    "        print(f\"Using cached vector store for topic: {topic}\")\n",
    "        return VECTOR_STORE_CACHE[topic]\n",
    "    \n",
    "    # Process URLs and create a new vector store\n",
    "    docs = process_urls(urls)\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "    \n",
    "    try:\n",
    "        vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "        VECTOR_STORE_CACHE[topic] = vectorstore  # Cache it for future use\n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def answer_question(question: str) -> str:\n",
    "    try:\n",
    "        # Use SOTA topic extraction to check for predefined topics\n",
    "        topic = extract_topic(question)\n",
    "        \n",
    "        if topic:\n",
    "            print(f\"Extracted topic: {topic}\")\n",
    "            urls = PREDEFINED_URLS[topic]\n",
    "            vectorstore = get_vector_store_for_topic(topic, urls)\n",
    "            \n",
    "            if vectorstore:\n",
    "                retriever = vectorstore.as_retriever(k=3)\n",
    "                context_docs = retriever.get_relevant_documents(question)\n",
    "                return answer_with_context(question, context_docs)\n",
    "            else:\n",
    "                return \"Failed to retrieve information for the topic.\"\n",
    "        else:\n",
    "            # Optionally, you can perform a web search if the topic is not supported\n",
    "            print(\"Query topic is not supported. Skipping response.\")\n",
    "            return \"The query topic is not supported as it doesn't match any predefined topics.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {str(e)}\")\n",
    "        return \"An error occurred while processing your request.\"\n",
    "\n",
    "\n",
    "# First query on AI safety (this will create and cache the vector store)\n",
    "question1 = \"Tell me about quantum computing\"\n",
    "print(\"Answer 1:\", answer_question(question1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted topic: quantum computing\n",
      "Using cached vector store for topic: quantum computing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vamsv\\dbs\\lib\\site-packages\\langsmith\\client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 2: Qubits are the fundamental units of quantum computing, representing both 0 and 1 values simultaneously. They can exist in multiple states at once, known as a superposition, allowing for exponentially more processing power than classical computers. This property enables quantum computers to solve complex problems efficiently.\n"
     ]
    }
   ],
   "source": [
    "# Second query on the same topic (this will reuse the cached vector store)\n",
    "question2 = \"what are qubits in quantum computing?\"\n",
    "print(\"Answer 2:\", answer_question(question2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query topic is not supported. Skipping response.\n",
      "Answer 3: The query topic is not supported as it doesn't match any predefined topics.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query on an unsupported topic (no processing or caching occurs)\n",
    "question3 = \"What are the latest trends in fusion energy?\"\n",
    "print(\"Answer 3:\", answer_question(question3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Scholar Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain import hub\n",
    "import trafilatura\n",
    "\n",
    "# For SOTA topic extraction using sentence transformers\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# For our custom Google Scholar loader\n",
    "from scholarly import scholarly\n",
    "\n",
    "# Predefined list of webpages (format: {topic: [urls]})\n",
    "PREDEFINED_URLS = {\n",
    "    \"AI safety\": [\n",
    "        \"https://www.deepmind.com/about/safety-and-security\",\n",
    "        \"https://openai.com/safety\"\n",
    "    ],\n",
    "    \"quantum computing\": [\n",
    "        \"https://qiskit.org/learn/\",\n",
    "        \"https://quantum.microsoft.com/\"\n",
    "    ]\n",
    "}\n",
    "# List of topics for similarity comparisons\n",
    "PREDEFINED_TOPICS = list(PREDEFINED_URLS.keys())\n",
    "\n",
    "# Initialize the SOTA embedding model for topic extraction\n",
    "topic_extractor = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def extract_topic(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the most semantically similar topic from the predefined topics.\n",
    "    Returns the topic if the similarity score exceeds a threshold, else None.\n",
    "    \"\"\"\n",
    "    query_embedding = topic_extractor.encode(query, convert_to_tensor=True)\n",
    "    topics_embeddings = topic_extractor.encode(PREDEFINED_TOPICS, convert_to_tensor=True)\n",
    "    cosine_scores = util.cos_sim(query_embedding, topics_embeddings)\n",
    "    best_score, best_index = torch.max(cosine_scores, dim=1)\n",
    "    # Set a threshold (e.g., 0.6) for considering the topic as a match\n",
    "    if best_score.item() >= 0.6:\n",
    "        return PREDEFINED_TOPICS[best_index.item()]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def web_search(query):\n",
    "    \"\"\"Fallback web search using DuckDuckGo\"\"\"\n",
    "    search = DuckDuckGoSearchAPIWrapper()\n",
    "    return search.results(query, 3)  # Get top 3 results\n",
    "\n",
    "from typing import List\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def process_urls(urls: List[str]) -> List[Document]:\n",
    "    \"\"\"Process URLs into document chunks with error handling\"\"\"\n",
    "    documents = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            downloaded = trafilatura.fetch_url(url)\n",
    "            text = trafilatura.extract(downloaded)\n",
    "            if text:\n",
    "                documents.append(Document(\n",
    "                    page_content=text,\n",
    "                    metadata={\"source\": url}\n",
    "                ))\n",
    "                print(f\"Successfully processed: {url}\")\n",
    "            else:\n",
    "                print(f\"No content found at: {url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {str(e)}\")\n",
    "    \n",
    "    if not documents:\n",
    "        raise ValueError(\"No valid documents found from the provided URLs\")\n",
    "    \n",
    "    # Split documents into manageable chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "def process_google_scholar(topic: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Use the scholarly package to fetch scholarly documents on the topic,\n",
    "    then create Document objects.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    try:\n",
    "        search_query = scholarly.search_pubs(topic)\n",
    "        # Limit to a few results (e.g., top 3)\n",
    "        for i, result in enumerate(search_query):\n",
    "            if i >= 3:\n",
    "                break\n",
    "            bib = result.get('bib', {})\n",
    "            title = bib.get('title', 'No Title')\n",
    "            abstract = bib.get('abstract', 'No Abstract Available')\n",
    "            content = f\"Title: {title}\\nAbstract: {abstract}\"\n",
    "            documents.append(Document(\n",
    "                page_content=content,\n",
    "                metadata={\"source\": \"Google Scholar\"}\n",
    "            ))\n",
    "            print(f\"Fetched Google Scholar result: {title}\")\n",
    "        if not documents:\n",
    "            print(\"No documents found via Google Scholar.\")\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Google Scholar data: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Initialize LLM (Make sure Ollama is running)\n",
    "llm = Ollama(model=\"llama3.2:3b\")\n",
    "\n",
    "def answer_with_context(question: str, context_docs: List[Document]):\n",
    "    \"\"\"Generate answer using context documents\"\"\"\n",
    "    if not context_docs:\n",
    "        return \"No relevant information found to answer this question.\"\n",
    "    \n",
    "    # Create retrieval chain using the provided context\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "    try:\n",
    "        vectorstore = FAISS.from_documents(context_docs, embeddings)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {str(e)}\")\n",
    "        return \"Failed to process documents for answering.\"\n",
    "    \n",
    "    retriever = vectorstore.as_retriever(k=3)\n",
    "    \n",
    "    # Create QA chain using a predefined prompt\n",
    "    qa_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\n",
    "    combine_docs_chain = StuffDocumentsChain(\n",
    "        llm_chain=qa_chain, document_variable_name=\"context\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        result = combine_docs_chain.invoke({\n",
    "            \"input_documents\": context_docs,\n",
    "            \"question\": question\n",
    "        })\n",
    "        return result[\"output_text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer: {str(e)}\")\n",
    "        return \"Failed to generate answer.\"\n",
    "\n",
    "# --- Caching Strategy Implementation ---\n",
    "# Cache for topic-based FAISS vector stores\n",
    "VECTOR_STORE_CACHE = {}\n",
    "\n",
    "def get_vector_store_for_topic(topic: str, urls: List[str]):\n",
    "    \"\"\"\n",
    "    Retrieve or create a FAISS vector store for the given topic.\n",
    "    This function uses caching and downloads data from both predefined URLs\n",
    "    and Google Scholar.\n",
    "    \"\"\"\n",
    "    if topic in VECTOR_STORE_CACHE:\n",
    "        print(f\"Using cached vector store for topic: {topic}\")\n",
    "        return VECTOR_STORE_CACHE[topic]\n",
    "    \n",
    "    # Process documents from predefined URLs\n",
    "    docs_web = process_urls(urls)\n",
    "    # Process scholarly documents via our custom Google Scholar loader\n",
    "    docs_scholar = process_google_scholar(topic)\n",
    "    \n",
    "    # Combine documents from both sources\n",
    "    all_docs = docs_web + docs_scholar\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "    \n",
    "    try:\n",
    "        vectorstore = FAISS.from_documents(all_docs, embeddings)\n",
    "        VECTOR_STORE_CACHE[topic] = vectorstore  # Cache for future use\n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def answer_question(question: str) -> str:\n",
    "    try:\n",
    "        # Use SOTA topic extraction to check for a predefined topic\n",
    "        topic = extract_topic(question)\n",
    "        \n",
    "        if topic:\n",
    "            print(f\"Extracted topic: {topic}\")\n",
    "            urls = PREDEFINED_URLS[topic]\n",
    "            vectorstore = get_vector_store_for_topic(topic, urls)\n",
    "            \n",
    "            if vectorstore:\n",
    "                retriever = vectorstore.as_retriever(k=3)\n",
    "                context_docs = retriever.get_relevant_documents(question)\n",
    "                return answer_with_context(question, context_docs)\n",
    "            else:\n",
    "                return \"Failed to retrieve information for the topic.\"\n",
    "        else:\n",
    "            # If the topic is unsupported, do not process further.\n",
    "            print(\"Query topic is not supported. Skipping response.\")\n",
    "            return \"The query topic is not supported as it doesn't match any predefined topics.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {str(e)}\")\n",
    "        return \"An error occurred while processing your request.\"\n",
    "\n",
    "# --- Example usage ---\n",
    "\n",
    "# First query on AI safety (this will create and cache the vector store)\n",
    "question1 = \"Tell me about quantum computing\"\n",
    "print(\"Answer 1:\", answer_question(question1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second query on the same topic (this will reuse the cached vector store)\n",
    "question2 = \"what are qubits in quantum computing?\"\n",
    "print(answer_question(question2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query on an unsupported topic (no processing or caching occurs)\n",
    "question3 = \"What are the latest trends in fusion energy?\"\n",
    "print(answer_question(question3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query on an unsupported topic (no processing or caching occurs)\n",
    "question3 = \"What are quantum gates\"\n",
    "print(answer_question(question3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
